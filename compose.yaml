services:
  # HDFS NameNode (master)
  namenode:
    image: matnar/hadoop
    hostname: master
    container_name: namenode
    tty: true         # interactive
    stdin_open: true  # stdin open
    ports:
      - 9870:9870 # web-ui
    volumes:
      - ./data:/app/data        # folder containing the dataset
      - ./Results:/app/results  # folder for the results
    # it waits for the DataNodes to start
    depends_on:
      - datanode-1
      - datanode-2
      - datanode-3
  # HDFS DataNode
  datanode-1:
    image: matnar/hadoop
    container_name: datanode-1
    hostname: slave1
    tty: true
    stdin_open: true
  # HDFS DataNode
  datanode-2:
    image: matnar/hadoop
    container_name: datanode-2
    hostname: slave2
    tty: true
    stdin_open: true
  # HDFS DataNode
  datanode-3:
    image: matnar/hadoop
    container_name: datanode-3
    hostname: slave3
    tty: true
    stdin_open: true
  # Spark Master node
  spark-master:
    image: spark:python3
    container_name: spark-master
    hostname: spark-master
    command: /bin/bash
    tty: true
    stdin_open: true
    ports:
      - 8080:8080 # web-ui
      - 4040:4040 # web-ui
    depends_on:
      - namenode
    environment:
      SPARK_LOCAL_IP: spark-master
    volumes:
      - ./src:/app/
      - ./requirements.txt:/app/requirements.txt
      - ./config/nifi:/app/nifi
  # Spark Worker node
  spark-worker:
    image: spark:python3
    container_name: spark-worker
    hostname: spark-worker
    command: /bin/bash
    tty: true
    stdin_open: true
    depends_on:
      - spark-master
    env_file:
      - ./config/.env
  # Data Ingestion
  nifi:
    image: apache/nifi
    container_name: nifi
    hostname: nifi
    env_file:
      - ./config/.env
    environment:
      SINGLE_USER_CREDENTIALS_USERNAME: ${NIFI_USERNAME}
      SINGLE_USER_CREDENTIALS_PASSWORD: ${NIFI_PASSWORD}
    ports:
      - 8443:8443
    volumes:
      - ./config/hdfs:/data
    depends_on:
      - namenode
